{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "lIYdn1woOS1n"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "import re\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.layers import Input\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "QW1VpZH4ibG0"
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data = pd.read_csv(r'Sentiment.csv')\n",
    "\n",
    "# Keeping only the necessary columns\n",
    "data = data[['text', 'sentiment']]\n",
    "\n",
    "# Text preprocessing\n",
    "data['text'] = data['text'].apply(lambda x: x.lower())\n",
    "data['text'] = data['text'].apply(lambda x: re.sub(r'[^a-zA-Z0-9\\s]', '', x))\n",
    "\n",
    "# Replace 'rt' with a space\n",
    "for idx, row in data.iterrows():\n",
    "    data.iloc[idx, 0] = row['text'].replace('rt', ' ')\n",
    "\n",
    "# Tokenization and padding\n",
    "max_features = 2000\n",
    "tokenizer = Tokenizer(num_words=max_features, split=' ')\n",
    "tokenizer.fit_on_texts(data['text'].values)\n",
    "X = tokenizer.texts_to_sequences(data['text'].values)\n",
    "X = pad_sequences(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "XQv7iTlwjSkZ"
   },
   "outputs": [],
   "source": [
    "# Tokenization and padding\n",
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_features, split=' ')\n",
    "tokenizer.fit_on_texts(data['text'].values)\n",
    "X = tokenizer.texts_to_sequences(data['text'].values)\n",
    "X = pad_sequences(X)\n",
    "\n",
    "# Model creation with Input layer\n",
    "def createmodel():\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(X.shape[1],)))\n",
    "    model.add(Embedding(max_features, embed_dim))\n",
    "    model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# model = createmodel()\n",
    "# print(model.summary())\n",
    "\n",
    "# Encoding and splitting the data\n",
    "labelencoder = LabelEncoder()\n",
    "integer_encoded = labelencoder.fit_transform(data['sentiment'])\n",
    "y = to_categorical(integer_encoded)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HuhmEw_IjVo8",
    "outputId": "4751cc5a-d210-4229-d771-86864a7d18d0"
   },
   "outputs": [],
   "source": [
    "# Hyper Tune the model\n",
    "# Updated KerasClassifier with 'model' instead of 'build_fn'\n",
    "model = KerasClassifier(model=createmodel, verbose=0)\n",
    "\n",
    "batch_size = [32]\n",
    "epochs = [5]\n",
    "# optimizer = ['SGD','RMSprop','Adagrad','Adam']\n",
    "# activation = ['softmax','relu','tanh','sigmoid']\n",
    "# dropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid)\n",
    "grid_result = grid.fit(X_train, Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vAJnIxMtm4yL",
    "outputId": "99c10359-e6ca-48e1-d2d7-842d5fbab985"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params : 0.668241 using {'batch_size': 32, 'epochs': 5}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best params : %f using %s\"%(grid_result.best_score_,grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l-mFGCGOROpk",
    "outputId": "205cf5d2-d70a-4064-99d6-dd33bddde1fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "291/291 - 10s - 35ms/step - accuracy: 0.6404 - loss: 0.8312\n",
      "Epoch 2/5\n",
      "291/291 - 9s - 30ms/step - accuracy: 0.7053 - loss: 0.6966\n",
      "Epoch 3/5\n",
      "291/291 - 8s - 28ms/step - accuracy: 0.7295 - loss: 0.6291\n",
      "Epoch 4/5\n",
      "291/291 - 8s - 29ms/step - accuracy: 0.7562 - loss: 0.5819\n",
      "Epoch 5/5\n",
      "291/291 - 9s - 30ms/step - accuracy: 0.7800 - loss: 0.5401\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x17d74561820>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = grid_result.best_params_['batch_size']\n",
    "epochs = grid_result.best_params_['epochs']\n",
    "model = createmodel()\n",
    "model.fit(X_train, Y_train, epochs = epochs, batch_size=batch_size, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S2jWGCgBnBd9",
    "outputId": "b0d04645-0855-4e0b-a775-709124c157e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144/144 - 2s - 15ms/step - accuracy: 0.6785 - loss: 0.8084\n",
      "0.8083944320678711\n",
      "0.6784622073173523\n"
     ]
    }
   ],
   "source": [
    "score,acc = model.evaluate(X_test,Y_test,verbose=2,batch_size=batch_size)\n",
    "print(score)\n",
    "print(acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "oORub14kRaCB"
   },
   "outputs": [],
   "source": [
    "# test example case\n",
    "\n",
    "tweet = \"A lot of good things are happening. We are respected again throughout the world, and that's a great thing.@realDonaldTrump\"\n",
    "\n",
    "tweet = tweet.replace('rt',\"\")\n",
    "\n",
    "tweet_bg = tokenizer.texts_to_sequences(tweet)\n",
    "\n",
    "tweet_padded = pad_sequences(tweet_bg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GiFAACHUUNvu",
    "outputId": "89261c37-a77e-4754-c5ba-f241496d2edc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n"
     ]
    }
   ],
   "source": [
    "tweet_sent_pred = model.predict(tweet_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3R6Vyko1VGzw",
    "outputId": "1fb4590b-e143-4064-c932-75ea06cfe2f9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.55505395, 0.23779899, 0.2071471 ],\n",
       "       [0.32727835, 0.29615402, 0.3765676 ],\n",
       "       [0.32727835, 0.29615402, 0.3765676 ],\n",
       "       [0.7572455 , 0.19346455, 0.04928995],\n",
       "       [0.24874997, 0.41596603, 0.33528402],\n",
       "       [0.32727835, 0.29615402, 0.3765676 ],\n",
       "       [0.7572455 , 0.19346455, 0.04928995],\n",
       "       [0.10452683, 0.34663066, 0.54884243],\n",
       "       [0.32727835, 0.29615402, 0.3765676 ],\n",
       "       [0.5195091 , 0.21039966, 0.27009124],\n",
       "       [0.7572455 , 0.19346455, 0.04928995],\n",
       "       [0.7572455 , 0.19346455, 0.04928995],\n",
       "       [0.10364561, 0.3030908 , 0.59326357],\n",
       "       [0.32727835, 0.29615402, 0.3765676 ],\n",
       "       [0.24874997, 0.41596603, 0.33528402],\n",
       "       [0.11241672, 0.31100932, 0.576574  ],\n",
       "       [0.30262294, 0.37865   , 0.31872708],\n",
       "       [0.12434048, 0.62751055, 0.248149  ],\n",
       "       [0.5195091 , 0.21039966, 0.27009124],\n",
       "       [0.23313177, 0.48477122, 0.282097  ],\n",
       "       [0.32727835, 0.29615402, 0.3765676 ],\n",
       "       [0.55505395, 0.23779899, 0.2071471 ],\n",
       "       [0.7648134 , 0.14566073, 0.08952582],\n",
       "       [0.32727835, 0.29615402, 0.3765676 ],\n",
       "       [0.32727835, 0.29615402, 0.3765676 ],\n",
       "       [0.11241672, 0.31100932, 0.576574  ],\n",
       "       [0.55505395, 0.23779899, 0.2071471 ],\n",
       "       [0.32727835, 0.29615402, 0.3765676 ],\n",
       "       [0.32727835, 0.29615402, 0.3765676 ],\n",
       "       [0.32727835, 0.29615402, 0.3765676 ],\n",
       "       [0.12434048, 0.62751055, 0.24814896],\n",
       "       [0.30262294, 0.37865   , 0.31872708],\n",
       "       [0.12434048, 0.62751055, 0.248149  ],\n",
       "       [0.5195091 , 0.21039966, 0.27009124],\n",
       "       [0.32727835, 0.29615402, 0.3765676 ],\n",
       "       [0.32727835, 0.29615402, 0.3765676 ],\n",
       "       [0.3847201 , 0.50115937, 0.11412058],\n",
       "       [0.32727835, 0.29615402, 0.3765676 ],\n",
       "       [0.32727835, 0.29615402, 0.3765676 ],\n",
       "       [0.55505395, 0.23779899, 0.2071471 ],\n",
       "       [0.7648134 , 0.14566073, 0.08952582],\n",
       "       [0.32727835, 0.29615402, 0.3765676 ],\n",
       "       [0.32727835, 0.29615402, 0.3765676 ],\n",
       "       [0.7648134 , 0.14566073, 0.08952582],\n",
       "       [0.32727835, 0.29615402, 0.3765676 ],\n",
       "       [0.23313177, 0.48477122, 0.282097  ],\n",
       "       [0.32727835, 0.29615402, 0.3765676 ],\n",
       "       [0.32727835, 0.29615402, 0.3765676 ],\n",
       "       [0.26053882, 0.54421765, 0.19524358],\n",
       "       [0.24874997, 0.41596603, 0.33528402],\n",
       "       [0.32727835, 0.29615402, 0.3765676 ],\n",
       "       [0.10364561, 0.3030908 , 0.59326357],\n",
       "       [0.32727835, 0.29615402, 0.3765676 ],\n",
       "       [0.55505395, 0.23779899, 0.2071471 ],\n",
       "       [0.5195091 , 0.21039966, 0.27009124],\n",
       "       [0.55505395, 0.23779899, 0.2071471 ],\n",
       "       [0.30262294, 0.37865   , 0.31872708],\n",
       "       [0.12434048, 0.62751055, 0.248149  ],\n",
       "       [0.32727835, 0.29615402, 0.3765676 ],\n",
       "       [0.24874997, 0.41596603, 0.33528402],\n",
       "       [0.11241672, 0.31100932, 0.576574  ],\n",
       "       [0.7648134 , 0.14566073, 0.08952582],\n",
       "       [0.7572455 , 0.19346452, 0.04928994],\n",
       "       [0.42474636, 0.38827172, 0.18698196],\n",
       "       [0.5195091 , 0.21039966, 0.27009124],\n",
       "       [0.11241672, 0.31100932, 0.576574  ],\n",
       "       [0.7572455 , 0.19346455, 0.04928995],\n",
       "       [0.42474636, 0.38827172, 0.18698196],\n",
       "       [0.24874997, 0.41596603, 0.33528402],\n",
       "       [0.32727835, 0.29615402, 0.3765676 ],\n",
       "       [0.24874997, 0.41596603, 0.33528402],\n",
       "       [0.11241672, 0.31100932, 0.576574  ],\n",
       "       [0.32727835, 0.29615402, 0.3765676 ],\n",
       "       [0.32727835, 0.29615402, 0.3765676 ],\n",
       "       [0.3847201 , 0.50115937, 0.11412058],\n",
       "       [0.7572455 , 0.19346455, 0.04928995],\n",
       "       [0.7648134 , 0.14566073, 0.08952582],\n",
       "       [0.32727835, 0.29615402, 0.3765676 ],\n",
       "       [0.10364561, 0.3030908 , 0.59326357],\n",
       "       [0.32727835, 0.29615402, 0.3765676 ],\n",
       "       [0.32727835, 0.29615402, 0.3765676 ],\n",
       "       [0.55505395, 0.23779899, 0.2071471 ],\n",
       "       [0.12434048, 0.62751055, 0.248149  ],\n",
       "       [0.10364561, 0.3030908 , 0.59326357],\n",
       "       [0.32727835, 0.29615402, 0.3765676 ],\n",
       "       [0.24874997, 0.41596603, 0.33528402],\n",
       "       [0.11241672, 0.31100932, 0.576574  ],\n",
       "       [0.55505395, 0.23779899, 0.2071471 ],\n",
       "       [0.24874997, 0.41596603, 0.33528402],\n",
       "       [0.32727835, 0.29615402, 0.3765676 ],\n",
       "       [0.23313177, 0.48477122, 0.282097  ],\n",
       "       [0.32727835, 0.29615402, 0.3765676 ],\n",
       "       [0.55505395, 0.23779899, 0.2071471 ],\n",
       "       [0.32727835, 0.29615402, 0.3765676 ],\n",
       "       [0.5195091 , 0.21039967, 0.27009127],\n",
       "       [0.7648135 , 0.14566073, 0.08952583],\n",
       "       [0.32727835, 0.29615402, 0.3765676 ],\n",
       "       [0.55505395, 0.23779899, 0.2071471 ],\n",
       "       [0.24874997, 0.41596603, 0.33528402],\n",
       "       [0.32727835, 0.29615402, 0.3765676 ],\n",
       "       [0.24874997, 0.41596603, 0.33528402],\n",
       "       [0.11241672, 0.31100932, 0.576574  ],\n",
       "       [0.30262294, 0.37865   , 0.31872708],\n",
       "       [0.12434048, 0.62751055, 0.248149  ],\n",
       "       [0.5195091 , 0.21039966, 0.27009124],\n",
       "       [0.32727835, 0.29615402, 0.3765676 ],\n",
       "       [0.32727835, 0.29615402, 0.3765676 ],\n",
       "       [0.7648134 , 0.14566073, 0.08952582],\n",
       "       [0.32727835, 0.29615402, 0.3765676 ],\n",
       "       [0.55505395, 0.23779899, 0.2071471 ],\n",
       "       [0.32727835, 0.29615402, 0.3765676 ],\n",
       "       [0.10364561, 0.3030908 , 0.59326357],\n",
       "       [0.7572455 , 0.19346455, 0.04928995],\n",
       "       [0.12434048, 0.62751055, 0.248149  ],\n",
       "       [0.55505395, 0.23779899, 0.2071471 ],\n",
       "       [0.32727835, 0.29615402, 0.3765676 ],\n",
       "       [0.10364561, 0.3030908 , 0.59326357],\n",
       "       [0.24874997, 0.41596603, 0.33528402],\n",
       "       [0.7648134 , 0.14566073, 0.08952582],\n",
       "       [0.42474636, 0.38827172, 0.18698196],\n",
       "       [0.39269194, 0.48542523, 0.12188286],\n",
       "       [0.32727835, 0.29615402, 0.3765676 ]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_sent_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "D0AfTL51VT8X"
   },
   "outputs": [],
   "source": [
    "# model.save(\"model_sent.h5\") legacy : depricated\n",
    "# Saving model in the recommended .keras format\n",
    "model.save(\"model_sent.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "of0dncFeWoaL",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 64 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000017D733A39C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model once at the start of the script\n",
    "model = load_model(\"model_sent.keras\")\n",
    "\n",
    "# Define the prediction function without @tf.function\n",
    "def make_prediction(input_text):\n",
    "    sample_sequence = tokenizer.texts_to_sequences([input_text])\n",
    "    sample_padded = pad_sequences(sample_sequence, maxlen=X.shape[1])\n",
    "    predicted_class = model.predict(sample_padded)\n",
    "    predicted_label = labelencoder.inverse_transform([np.argmax(predicted_class)])\n",
    "    return predicted_label[0]\n",
    "\n",
    "# Sample usage\n",
    "sample_text = \"A lot of good things are happening. We are respected again throughout the world, and that's a great thing.\"\n",
    "predicted_sentiment = make_prediction(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "print(f\"Predicted sentiment: {predicted_sentiment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Video URL:- https://drive.google.com/file/d/1HcrPz2LFx9PL_KzgtKquUt-gqfAhLL5k/view?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "name": "scratchpad",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
